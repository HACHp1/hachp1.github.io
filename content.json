{"meta":{"title":"Drinking math with code,CHP的个人博客","subtitle":"Focus","description":"记录追逐时间的points与闪现的ideas。","author":"CHP","url":"http://hachp1.github.io"},"pages":[{"title":"about","date":"2018-04-04T13:18:45.000Z","updated":"2018-04-04T13:28:51.094Z","comments":true,"path":"about.html","permalink":"http://hachp1.github.io/about.html","excerpt":"","text":"ĳ��ѧѧɮ��ѧϰMLing��ѧϰWeb��ȫing�� ɶ����ѧ��ɶ����ѧ�ǲ����ܵģ��Ⱳ�Ӷ������ܵģ�ѧһ���ֲ��ᣬ��ֻ��ѧ��������ά�ֵ������������ӡ�"},{"title":"�鵵","date":"2017-10-27T03:55:25.615Z","updated":"2017-10-27T03:55:25.615Z","comments":true,"path":"archive/index.html","permalink":"http://hachp1.github.io/archive/index.html","excerpt":"","text":""},{"title":"Categories","date":"2018-04-03T02:14:02.555Z","updated":"2017-08-31T04:13:03.000Z","comments":true,"path":"categories/index.html","permalink":"http://hachp1.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-04-03T02:14:19.157Z","updated":"2017-08-31T04:13:03.000Z","comments":true,"path":"tags/index.html","permalink":"http://hachp1.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Fwaf短源码学习","slug":"Fwaf短源码学习","date":"2018-04-08T12:23:59.000Z","updated":"2018-04-08T12:47:30.794Z","comments":true,"path":"posts/机器学习/20180408-ml.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180408-ml.html","excerpt":"","text":"Fwaf Fwaf是GitHub上的一个机器学习Web恶意请求防火墙，代码比较简洁，思路也比较清晰，由于和自己的某个想法很契合，就稍作分析。 库的使用和特征化 Fwaf使用sklearn库训练样本集。 其中，使用TfidfVectorizer对字符串进行特征化。 TfidfVectorizer TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，互联网上的搜索引擎还会使用基于连结分析的评级方法，以确定文件在搜寻结果中出现的顺序。 TF: Term Frequency \\[ TF(t)=\\frac{n(character(i))}{n(character)} \\] IDF: 逆向文件频率，用于衡量一个词的重要性 \\[ ID(t) = log_e(\\frac{numOfDocs}{numOfDocsWhichIncludeT}) \\] \\[ TF-IDF = TF*IDF \\] sklearn.feature_extraction.text.TfidfVectorizer：可以将文档转换成TF-IDF特征的矩阵。 1vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3)) 样本集 作者使用txt格式的样本集，每一行为一个样本，分&quot;goodqueries.txt&quot;和&quot;badqueries.txt&quot;。 截取几行： 123/top.php?stuff='uname &gt;q36497765 #/h21y8w52.nsf?&lt;script&gt;cross_site_scripting.nasl&lt;/script&gt;/ca000001.pl?action=showcart&amp;hop=\\\"&gt;&lt;script&gt;alert('vulnerable') 机器学习算法 作者使用逻辑回归算法，算法虽然简单，但效果还比较好:) 比较适合刚写机器学习代码不久的我:) 1lgs = LogisticRegression(class_weight=&#123;1: 2 * validCount / badCount, 0: 1.0&#125;) 1234Accuracy: 0.999420Precision: 0.984403Recall: 0.998520F1-Score: 0.991411 各项指标都很高啊2333 贴上作者源代码： 源代码链接：Fwaf-Machine-Learning-driven-Web-Application-Firewall","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/tags/机器学习/"}]},{"title":"terminal和VIM的分屏简单命令","slug":"terminal和VIM的分屏简单命令","date":"2018-04-06T14:05:33.813Z","updated":"2018-04-08T11:07:05.419Z","comments":true,"path":"posts/Linux日常/20180406-vim&t.html","link":"","permalink":"http://hachp1.github.io/posts/Linux日常/20180406-vim&t.html","excerpt":"","text":"Linux下的分屏 在远程登陆Linux时，要远程启动多个程序，分屏显得很重要。 在这里小记一下几种简单的分屏命令。 1、terminal分屏 使用tmux对terminal分屏，常用指令如下： 开启tmux：在terminal中输入tmux开启分屏。 tmux ls： 显示已有的tmux会话 tmux attach-session -t 数字： 选择tmux tmux kill-session -t session-name：关闭tmux [ctrl+b]为tmux的指令输入前缀，以下指令为输入前缀指令后的指令： 上下分屏：&quot; 左右分屏: % 切换屏幕：o 关闭一个终端：x 上下分屏与左右分屏切换：空格键 还可以调整分屏大小（平均化） 显示快捷键帮助：？ 移动到下一个窗口：n 貌似比较鸡肋 显示时钟：t （ps：显示效果还可以） 2、VIM分屏 载入文件 在新的垂直分屏中打开文件:vs 文件路径/文件名 在新的水平分屏中打开文件:sv 文件路径/文件名 与tmux类似，[ctrl+w]为VIM的指令输入前缀，以下指令为输入指令前缀后的指令： 下一个分屏：w 上一个分屏：p 新建分屏：n（new）貌似比较鸡肋 水平分屏：s（split）貌似也比较鸡肋 垂直分屏：v（vsplit) 还是比较鸡肋 关闭分屏：c（close）或者直接命令模式 :q 使用 hjkl选择分屏 具体还有其他指令，不会再查，感觉够用了，懒得记:)","categories":[{"name":"Linux日常","slug":"Linux日常","permalink":"http://hachp1.github.io/categories/Linux日常/"}],"tags":[{"name":"Linux日常","slug":"Linux日常","permalink":"http://hachp1.github.io/tags/Linux日常/"}]},{"title":"林轩田机器学习基石 第一周、第二周","slug":"林轩田机器学习基石笔记-第一周、第二周","date":"2018-04-03T11:34:44.000Z","updated":"2018-04-07T02:48:42.042Z","comments":true,"path":"posts/机器学习/20180403-ml1.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180403-ml1.html","excerpt":"","text":"第一周，基本概念 机器学习可以进行的条件： 1、 有某种模式可以学习。 2、 这种模式不知道怎么手工明确规定（如果通过编写可以实现的就不需要机器学习）。 3、 有数据资料。 机器学习四种元素： 1、 输入X。 2、 输出Y。 3、 hypothesis H。 4、 资料 D 数据挖掘和机器学习有很多重合点，但不是一模一样。 第二周，二元是非判断 Perceptron Hypothesis：感知器学习算法：针对线性可分的资料 - PLA算法（perceptron learning algorithm），两种理解方式： ① 向量纠正（比较直观，但证明很麻烦，推导不方便） 当y=+1，那么类似于第一个图，w+yx将使得新的w更加偏向于x，以使得修正后的结果为h(x)&gt;0，而类似有第二图的修正。 ② 梯度下降（以点到直线的距离（带符号）为代价函数进行随机梯度下降）此方法可获得向量纠正的一样的公式及结果。 此法以误分类点到直线的距离为代价函数，使用随机梯度下降获得最优解，因如果误分类点到超平面的距离都最小时，则误分类点在线性可分的情况下变为正确分类点。 算法特性：只能分类线性可分的模型。 - 噪音相对于数据应该较小 - PLA变形：pocket algorithm，速度比PLA慢: 使用一个随机的g0作为起始，存贮目前为止代价函数最小的情况，迭代规定的若干次后得到结果。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/tags/机器学习/"}]},{"title":"江城子 随想","slug":"随想","date":"2018-04-02T17:18:55.000Z","updated":"2018-04-04T12:26:06.858Z","comments":true,"path":"posts/杂文/20180403-thinking.html","link":"","permalink":"http://hachp1.github.io/posts/杂文/20180403-thinking.html","excerpt":"","text":"晨日暖阳斜倚窗。绿树桩，白屋房。 微风过面，虫鸣燕正忙。城中伊人早登墙，红笑靥，含蓄望。 料知心事不可想。蛾眉锁，轻拂妆。 路客打量，何事溢心房。遥寄命途多渴望，手心暖，思却凉。","categories":[{"name":"杂文","slug":"杂文","permalink":"http://hachp1.github.io/categories/杂文/"}],"tags":[{"name":"心情随笔","slug":"心情随笔","permalink":"http://hachp1.github.io/tags/心情随笔/"}]},{"title":"重拾HEXO","slug":"重拾HEXO","date":"2018-04-02T15:27:50.000Z","updated":"2018-04-04T12:27:05.770Z","comments":true,"path":"posts/uncategorized/20180402-return.html","link":"","permalink":"http://hachp1.github.io/posts/uncategorized/20180402-return.html","excerpt":"","text":"似乎从来没有开始过 之后会常来的 抛弃word 23333","categories":[],"tags":[]}]}