{"meta":{"title":"Drinking math with code,CHP的个人博客","subtitle":"Focus","description":"记录追逐时间的points与闪现的ideas。","author":"CHP","url":"http://hachp1.github.io"},"pages":[{"title":"about","date":"2018-04-04T13:18:45.000Z","updated":"2018-04-04T13:28:51.094Z","comments":true,"path":"about.html","permalink":"http://hachp1.github.io/about.html","excerpt":"","text":"ĳ��ѧѧɮ��ѧϰMLing��ѧϰWeb��ȫing�� ɶ����ѧ��ɶ����ѧ�ǲ����ܵģ��Ⱳ�Ӷ������ܵģ�ѧһ���ֲ��ᣬ��ֻ��ѧ��������ά�ֵ������������ӡ�"},{"title":"�鵵","date":"2017-10-27T03:55:25.615Z","updated":"2017-10-27T03:55:25.615Z","comments":true,"path":"archive/index.html","permalink":"http://hachp1.github.io/archive/index.html","excerpt":"","text":""},{"title":"Categories","date":"2018-04-03T02:14:02.555Z","updated":"2017-08-31T04:13:03.000Z","comments":true,"path":"categories/index.html","permalink":"http://hachp1.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-04-03T02:14:19.157Z","updated":"2017-08-31T04:13:03.000Z","comments":true,"path":"tags/index.html","permalink":"http://hachp1.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"杂·sklearn 保存训练结果","slug":"杂·SKlearn训练结果保存","date":"2018-05-05T09:23:18.000Z","updated":"2018-05-05T09:27:38.932Z","comments":true,"path":"posts/机器学习/20180505-dump.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180505-dump.html","excerpt":"","text":"一、使用joblib保存结果 代码如下： 123456789from sklearn.externals import joblib#此处假设已训练好的模型为learnClassifier#保存训练模型joblib.dump(learnClassifier, 'learnClassifier.model')#载入训练模型learnClassifier = joblib.load('learnClassifier.model') 二、使用pickle保存结果 与joblib相比，pickle实际上是序列化和反序列化的函数。 注：pickle函数加s表示在bytes层面（程序变量中）的操作，而不加s的则是对文件的操作。 代码如下： 1234567#保存训练模型dump=pickle.dumps(classifier)with open(&apos;classifier_dump.pickle&apos;,&apos;w&apos;) as f: f.write(dump)#载入训练模型with open(&apos;classifier_dump.pickle&apos;,&apos;r&apos;) as f: classifier=pickle.load(f.read())","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/tags/机器学习/"}]},{"title":"对逻辑回归的进一步理解","slug":"对逻辑回归的进一步理解","date":"2018-04-11T14:32:52.000Z","updated":"2018-04-23T13:58:26.632Z","comments":true,"path":"posts/机器学习/20180411-logistic.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180411-logistic.html","excerpt":"","text":"概要 逻辑回归即使用一个超平面将整个空间一分为二，并且在边界附近赋予连续（即不是非黑即白，而是可以有0-1之间的小数概率）的概率估计的二分类方法。 逻辑回归使用\\(θX\\)作为划分数据的边界。 逻辑回归使用 \\[ g( Z) =\\frac{1}{1+e^{Z}} \\] 替代离散的分布函数，其中，g(Z)即概率统计中的分布函数，可根据极大似然估计得到使用已有样本点估计的近似值：从理论上来讲，此时的似然函数应该是最大的，可以用梯度上升法求之。 对边界的理解 显然，Z=θX这个“超平面”可以作为划分整个空间的依据。本来到这里就可以用分段函数完成估计了，但是（1）分布函数是离散的，（2）并且不利于计算靠近分界时的概率，所以引入了S函数使分布函数连续化且能估计在边界附近的概率值，一举两得。 对代价函数的证明与理解 在证明的时候，本来是要分别讨论y=1和y=0时的分布函数的，在得出代价函数的时候使用了一个技巧统一了公式 我们知道，在实际情况下的点为\\((y_{i},X_{i})\\)，而\\(y_i\\)只能为0或1,所以有如下讨论： \\[h(X)=p(y=1|X_{i},θ)\\] 显然，只有以上这个等式我们并不能用到y=0的点，所以有如下公式： \\[1-h(X)=p(y=0|X_{i},θ)\\] 二者相乘后就得到了统一后的结果： \\[ h(X_{i})^{y_{i}}*(1-h(X_{i})^{1-y_{i}}=p(y_{i}|X_{i},θ) \\] 我们的目的就是要使以上式子最大化（极大似然的原理：我们观测到的一组数据是n个相互独立的随机变量的观测值，将它们出现的概率乘起来即得到整个观测值出现的概率，而我们观测到的现象一定是出现概率最大的那个结果，所以带入数据后整个式子的值最大）。 接下来就是熟悉的极大似然估计的步骤了，由极大似然估计法要将其最大化，利用高数知识，函数取对数后求导，令导数为零可以解出θ值(而在计算机中则使用梯度上升等方法得到结果） 对其取对数可得逻辑回归的代价函数（与其他回归的代价函数类似，但此处因为是极大似然估计，是求其最大值） \\[ L(θ)=-\\frac{1}{m}[\\sum_{i-1}^{m}y^{i}logh_{θ}(x^{i}+(1-y^{i})log(1-h_{θ}(x^{i}))] \\] 接下来就是手工求导出公式并使用梯度下降等算法了，可以愉快的逻辑回归了:) 对\\(θ_{i}\\)求偏导： \\[ \\frac{\\partial}{\\partial{θ_{i}} }L(θ)=(y-h(x))x_{i} \\] 梯度上升: \\[ θ_{i}=θ_{i}+α(y-h(x))x_{i} \\] 贴上手写的浮肿的代码（文件已传github）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import numpy as npimport matplotlib.pylab as plotclass Logistic: ''' 特征系数θ ''' theta = np.zeros((1, 1)) ''' 梯度上升循环次数 ''' cycleNum = 10000 ''' 特征向量X、标记向量y ''' X = np.zeros((1, 1)) Y = np.zeros((1, 1)) alpha = 1 def z(self, X): # z函数，决定z函数的形式 注：这里的X是向量不是矩阵 return X.dot(self.theta.transpose()) def h(self, x): return 1.0 / (1 + np.exp(-self.z(x))) def fit(self, X, Y): cx=np.ones((X.shape[0],1)) self.X = np.c_[cx,X] self.Y = Y self.theta = np.random.random((1, self.X.shape[1])) # 由于theta使用random函数导致其为二维而不是一维。 i = 0 j = 0 while j &lt; self.cycleNum: dtheta = np.zeros((1, self.X.shape[1])) # print(self.theta) # print(self.theta[0][0] / self.theta[0][1]) while i &lt;= self.Y.shape[0] - 1: dtheta += (self.Y[i] - self.h(self.X[i])) * self.X[i] i += 1 i = 0 # 初始化i self.theta = self.theta + self.alpha * dtheta j += 1 def predict(self, vX): output = self.h(vX) if output &lt; 0.5: return 0 else: return 1if __name__ == '__main__': lineSplit = [] x_train = [] y_train = [] with open(\"testSet-LR.txt\", 'r') as f: lines = f.readlines() for line in lines: lineSplit = (line.strip().split()) x_train.append([float(lineSplit[0]), float(lineSplit[1])]) y_train.append([int(lineSplit[2])]) x_train = np.array(x_train) y_train = np.array(y_train) logis = Logistic() logis.alpha = 100 logis.cycleNum = 30000 logis.fit(x_train, y_train) xop = [] yop = [] xpe = [] ype = [] i = 0 while i &lt;= x_train.shape[0] - 1: if y_train[i] == 1: xop.append(x_train[i][0]) yop.append(x_train[i][1]) else: xpe.append(x_train[i][0]) ype.append(x_train[i][1]) i += 1 fig=plot.figure() plot.scatter(xop, yop, color=\"red\") plot.scatter(xpe, ype, color=\"blue\") plot.xlim((-10,20 )) plot.ylim((-10, 20)) X = np.linspace(-10, 10, 30) Y = -X * logis.theta[0][1] / logis.theta[0][2] - logis.theta[0][0] / logis.theta[0][2] plot.plot(X, Y) plot.show() fig.savefig('lr.jpg') 参考资料：用最大似然估计求逻辑回归参数","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/tags/机器学习/"}]},{"title":"Fwaf短源码学习","slug":"Fwaf短源码学习","date":"2018-04-08T12:23:59.000Z","updated":"2018-04-09T09:30:50.597Z","comments":true,"path":"posts/机器学习/20180408-ml.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180408-ml.html","excerpt":"","text":"Fwaf Fwaf是GitHub上的一个机器学习Web恶意请求防火墙，代码比较简洁，思路也比较清晰，由于和自己的某个想法很契合，就稍作分析。 库的使用和特征化 Fwaf使用sklearn库训练样本集。 其中，使用TfidfVectorizer对字符串进行特征化。 TfidfVectorizer TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，互联网上的搜索引擎还会使用基于连结分析的评级方法，以确定文件在搜寻结果中出现的顺序。 TF: Term Frequency \\[ TF(t)=\\frac{n(character(i))}{n(character)} \\] IDF: 逆向文件频率，用于衡量一个词的重要性 \\[ ID(t) = log_e(\\frac{numOfDocs}{numOfDocsWhichIncludeT}) \\] \\[ TF-IDF = TF*IDF \\] sklearn.feature_extraction.text.TfidfVectorizer：可以将文档转换成TF-IDF特征的矩阵。 ngram_range：词组切分的长度范围。该范围之内的n元feature都会被提取出来，这个参数要根据自己的需求调整。 1vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3)) 样本集 作者使用txt格式的样本集，每一行为一个样本，分&quot;goodqueries.txt&quot;和&quot;badqueries.txt&quot;。 截取几行： 123/top.php?stuff='uname &gt;q36497765 #/h21y8w52.nsf?&lt;script&gt;cross_site_scripting.nasl&lt;/script&gt;/ca000001.pl?action=showcart&amp;hop=\\\"&gt;&lt;script&gt;alert('vulnerable') 机器学习算法 作者使用逻辑回归算法，算法虽然简单，但效果还比较好:) 比较适合刚写机器学习代码不久的我:) 1lgs = LogisticRegression(class_weight=&#123;1: 2 * validCount / badCount, 0: 1.0&#125;) 1234Accuracy: 0.999420Precision: 0.984403Recall: 0.998520F1-Score: 0.991411 各项指标都很高啊2333 贴上作者源代码： 源代码链接：Fwaf-Machine-Learning-driven-Web-Application-Firewall","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/tags/机器学习/"}]},{"title":"terminal和VIM的分屏简单命令","slug":"terminal和VIM的分屏简单命令","date":"2018-04-06T14:05:33.813Z","updated":"2018-05-05T09:45:11.859Z","comments":true,"path":"posts/Linux日常/20180406-vim&t.html","link":"","permalink":"http://hachp1.github.io/posts/Linux日常/20180406-vim&t.html","excerpt":"","text":"Linux下的分屏 在远程登陆Linux时，要远程启动多个程序，分屏显得很重要。 在这里小记一下几种简单的分屏命令。 1、terminal分屏 使用tmux对terminal分屏，常用指令如下： 开启tmux：在terminal中输入tmux开启分屏。 tmux ls： 显示已有的tmux会话 tmux attach-session -t 数字： 选择tmux tmux kill-session -t session-name：关闭tmux 开启鼠标移动、调节窗口大小等功能：[ctrl]+b+:后输入 set -g mouse on [ctrl+b]为tmux的指令输入前缀，以下指令为输入前缀指令后的指令： 上下分屏：&quot; 左右分屏: % 切换屏幕：o 关闭一个终端：x 上下分屏与左右分屏切换：空格键 还可以调整分屏大小（平均化） 显示快捷键帮助：？ 移动到下一个窗口：n 貌似比较鸡肋 显示时钟：t （ps：显示效果还可以） 临时退出session: d 列出session：tmux ls （不用前缀） 进入已存在的session：tmux a -t $session_name 关闭并删除所有session:[：]+ kill-server 复制模式 : [ 空格标记复制开始，回车结束复制。 粘贴 ：] 2、VIM分屏 载入文件 在新的垂直分屏中打开文件:vs 文件路径/文件名 在新的水平分屏中打开文件:sv 文件路径/文件名 与tmux类似，[ctrl+w]为VIM的指令输入前缀，以下指令为输入指令前缀后的指令： 下一个分屏：w 上一个分屏：p 使用 hjkl选择分屏 新建分屏：n（new）貌似比较鸡肋 水平分屏：s（split）貌似也比较鸡肋 垂直分屏：v（vsplit) 还是比较鸡肋 关闭分屏：c（close）或者直接命令模式 :q 具体还有其他指令，不会再查，感觉够用了，懒得记:)","categories":[{"name":"Linux日常","slug":"Linux日常","permalink":"http://hachp1.github.io/categories/Linux日常/"}],"tags":[{"name":"Linux日常","slug":"Linux日常","permalink":"http://hachp1.github.io/tags/Linux日常/"}]},{"title":"林轩田机器学习基石 第一周、第二周","slug":"林轩田机器学习基石笔记-第一周、第二周","date":"2018-04-03T11:34:44.000Z","updated":"2018-04-07T02:48:42.042Z","comments":true,"path":"posts/机器学习/20180403-ml1.html","link":"","permalink":"http://hachp1.github.io/posts/机器学习/20180403-ml1.html","excerpt":"","text":"第一周，基本概念 机器学习可以进行的条件： 1、 有某种模式可以学习。 2、 这种模式不知道怎么手工明确规定（如果通过编写可以实现的就不需要机器学习）。 3、 有数据资料。 机器学习四种元素： 1、 输入X。 2、 输出Y。 3、 hypothesis H。 4、 资料 D 数据挖掘和机器学习有很多重合点，但不是一模一样。 第二周，二元是非判断 Perceptron Hypothesis：感知器学习算法：针对线性可分的资料 - PLA算法（perceptron learning algorithm），两种理解方式： ① 向量纠正（比较直观，但证明很麻烦，推导不方便） 当y=+1，那么类似于第一个图，w+yx将使得新的w更加偏向于x，以使得修正后的结果为h(x)&gt;0，而类似有第二图的修正。 ② 梯度下降（以点到直线的距离（带符号）为代价函数进行随机梯度下降）此方法可获得向量纠正的一样的公式及结果。 此法以误分类点到直线的距离为代价函数，使用随机梯度下降获得最优解，因如果误分类点到超平面的距离都最小时，则误分类点在线性可分的情况下变为正确分类点。 算法特性：只能分类线性可分的模型。 - 噪音相对于数据应该较小 - PLA变形：pocket algorithm，速度比PLA慢: 使用一个随机的g0作为起始，存贮目前为止代价函数最小的情况，迭代规定的若干次后得到结果。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://hachp1.github.io/tags/机器学习/"}]},{"title":"江城子 随想","slug":"随想","date":"2018-04-02T17:18:55.000Z","updated":"2018-04-04T12:26:06.858Z","comments":true,"path":"posts/杂文/20180403-thinking.html","link":"","permalink":"http://hachp1.github.io/posts/杂文/20180403-thinking.html","excerpt":"","text":"晨日暖阳斜倚窗。绿树桩，白屋房。 微风过面，虫鸣燕正忙。城中伊人早登墙，红笑靥，含蓄望。 料知心事不可想。蛾眉锁，轻拂妆。 路客打量，何事溢心房。遥寄命途多渴望，手心暖，思却凉。","categories":[{"name":"杂文","slug":"杂文","permalink":"http://hachp1.github.io/categories/杂文/"}],"tags":[{"name":"心情随笔","slug":"心情随笔","permalink":"http://hachp1.github.io/tags/心情随笔/"}]},{"title":"重拾HEXO","slug":"重拾HEXO","date":"2018-04-02T15:27:50.000Z","updated":"2018-04-04T12:27:05.770Z","comments":true,"path":"posts/uncategorized/20180402-return.html","link":"","permalink":"http://hachp1.github.io/posts/uncategorized/20180402-return.html","excerpt":"","text":"似乎从来没有开始过 之后会常来的 抛弃word 23333","categories":[],"tags":[]}]}